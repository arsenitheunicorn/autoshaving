{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение языка и VK API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании вам нужно будет:\n",
    "\n",
    "* используя API Вконтакте, скачать комментарии к первым ста постам из пяти сообществ\n",
    "* натренировать модель распознавания языков на статьях из Википедии.\n",
    "* распознать язык всех комментариев, где в тексте есть 10 и более символов, и построить статистику"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VK API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подключения к ВКонтакте мы будем использовать VK API. Здесь есть документация к этой библиотеке https://vk-api.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = ''\n",
    "\n",
    "params = {'access_token': TOKEN,\n",
    "          'v': '5.95'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### К сожалению, документация предоженной вами библиотеки не дала мне уверенности в том, что мой пароль не попадет туда, куда я не хочу. Поэтому мой код работает на токене vk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vk(method, params):\n",
    "    api_req = 'https://api.vk.com/method/'\n",
    "    api_req += method + '?'\n",
    "    api_req += '&'.join(['{}={}'.format(key, params[key]) for key in params])\n",
    "    json_raw = requests.get(api_req).text\n",
    "    return json.loads(json_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список domain'ов, чтобы вам не копировать их самими :) спасибо :***\n",
    "publics = [\"futureisnow\",\n",
    "           \"eternalclassic\",\n",
    "           \"ukrlit_memes\",\n",
    "           \"ukrainer_net\",\n",
    "           \"amanzohel\",\n",
    "           \"barg_kurumk_culture\"]\n",
    "\n",
    "g_dmns = ','.join(publics)\n",
    "gidpars = params.copy()\n",
    "gidpars['group_ids'] = g_dmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-111587102: 'киберпанк, который мы заслужили', -129440544: 'eternal classic', -131348832: 'Файні меми про українську лiтературу', -5164516: 'Ukraїner', -66347916: 'Буряад аман зохёол', -95189895: 'Баргажанай буряадууд (Хурамхаан, Баргажан)'}\n"
     ]
    }
   ],
   "source": [
    "rawids = vk('groups.getById', gidpars)\n",
    "g_ids = {}\n",
    "for i in rawids['response']:\n",
    "    gid = -1 * i['id']\n",
    "    name = i['name']\n",
    "    g_ids[gid] = name\n",
    "    \n",
    "print(g_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wall_get(params):\n",
    "    m = 'wall.get'\n",
    "    posts = {}\n",
    "    result = vk(m, params)\n",
    "    items = result['response']['items']\n",
    "    p_ids = []\n",
    "    for p in items:\n",
    "        p_ids.append(p[\"id\"])\n",
    "    return p_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_corpora = {}\n",
    "\n",
    "params['count'] = '100'\n",
    "\n",
    "for dom in g_ids:\n",
    "    params['owner_id'] = str(dom)\n",
    "    posts = wall_get(params)\n",
    "    pre_corpora[dom] = posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_get(params, posts):\n",
    "    m = 'wall.getComments'\n",
    "    comments = []\n",
    "    for post_id in posts:\n",
    "        params['post_id'] = post_id\n",
    "        result = vk(m, params)\n",
    "        items = result['response']['items']\n",
    "        for item in items:\n",
    "            if 'text' in item.keys():\n",
    "                text = item['text']\n",
    "                if len(text) > 10:\n",
    "                    comments.append(text)\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180\tкиберпанк, который мы заслужили\n",
      "728\teternal classic\n",
      "249\tФайні меми про українську лiтературу\n",
      "199\tUkraїner\n",
      "255\tБуряад аман зохёол\n",
      "232\tБаргажанай буряадууд (Хурамхаан, Баргажан)\n"
     ]
    }
   ],
   "source": [
    "corpora = {}\n",
    "\n",
    "for dom in pre_corpora:\n",
    "    params['owner_id'] = str(dom)\n",
    "    p_ids = pre_corpora[dom]\n",
    "    posts = comments_get(params, p_ids)\n",
    "    corpora[g_ids[dom]] = posts\n",
    "    \n",
    "for c in corpora:\n",
    "    print('%d\\t%s' % (len(corpora[c]), c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренировка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наших комментариях встречались русский, украинский, английский и бурятский."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_langs = ('ru', 'uk', 'en', 'bxr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте документы, на которых вы будете обучать свои модели. Для наших целей хорошо иметь для каждого языка корпус размером около 50 статей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=40):\n",
    "    wiki_content = []\n",
    "    wikipedia.set_lang(lang)\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "            wiki_content.append(\"%s\\n%s\" % (page.title, page.content.replace('=', '')))\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            pass\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "wiki_texts = {}\n",
    "\n",
    "for lang in test_langs:\n",
    "    wcon = get_texts_for_lang(lang)\n",
    "    wiki_texts[lang] = wcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте определялку на частотах слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_freqlist(wiki_pages, max_len=100):\n",
    "    freqlist = Counter()\n",
    "    # не забудем про токенизацию - nltk.word_tokenize\n",
    "    for text in wiki_pages:\n",
    "        for word in nltk.word_tokenize(text.lower()):\n",
    "            if word.isalpha():\n",
    "                freqlist[word] += 1\n",
    "    return dict(freqlist.most_common(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = {}\n",
    "\n",
    "\n",
    "for lang in test_langs:\n",
    "    freqs[lang] = collect_freqlist(wiki_texts[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lang_detect(freq_lists, text):\n",
    "    counts = Counter()\n",
    "    for lang, freq_list in freq_lists.items():\n",
    "        freq_list = Counter(freq_list)\n",
    "        for word in nltk.word_tokenize(text):\n",
    "            counts[lang] += int(freq_list[word] > 0)\n",
    "    return counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте определялку на символьных энграммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib==3.0.3\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['text']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn import naive_bayes\n",
    "import numpy as np\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline.Pipeline([\n",
    "    ('vctr', feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2), analyzer='char')),\n",
    "    ('clf', naive_bayes.MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = []\n",
    "lang_indices = []\n",
    "for lang in wiki_texts:\n",
    "    all_texts.extend(wiki_texts[lang])\n",
    "    lang_indices.extend([lang]*len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vctr', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем классификатор\n",
    "clf.fit(np.array(all_texts), np.array(lang_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru',\n",
       "       'en', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru',\n",
       "       'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru', 'ru',\n",
       "       'ru', 'ru', 'ru', 'ru', 'ru', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk',\n",
       "       'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk',\n",
       "       'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk',\n",
       "       'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'uk', 'en', 'en',\n",
       "       'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en',\n",
       "       'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en',\n",
       "       'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en',\n",
       "       'en', 'ru', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr',\n",
       "       'bxr', 'bxr', 'ru', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr',\n",
       "       'bxr', 'bxr', 'bxr', 'ru', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr',\n",
       "       'bxr', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr', 'bxr',\n",
       "       'bxr', 'bxr', 'bxr', 'bxr'], dtype='<U3')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(all_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определите язык каждого комментария в каждом паблике с помощью определялки на частотах слов и покажите доли языков среди комментариев для каждого паблика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\tкиберпанк, который мы заслужили\n",
      "\n",
      "ru\t1161\t0.98\n",
      "uk\t7\t0.01\n",
      "en\t12\t0.01\n",
      "bxr\t0\t0.0\n",
      "total:\t1180\t1.00\n",
      "\n",
      "---\teternal classic\n",
      "\n",
      "ru\t293\t0.4\n",
      "uk\t0\t0.0\n",
      "en\t429\t0.59\n",
      "bxr\t5\t0.01\n",
      "total:\t727\t1.00\n",
      "\n",
      "---\tФайні меми про українську лiтературу\n",
      "\n",
      "ru\t168\t0.67\n",
      "uk\t77\t0.31\n",
      "en\t4\t0.02\n",
      "bxr\t0\t0.0\n",
      "total:\t249\t1.00\n",
      "\n",
      "---\tUkraїner\n",
      "\n",
      "ru\t122\t0.61\n",
      "uk\t76\t0.38\n",
      "en\t0\t0.0\n",
      "bxr\t1\t0.01\n",
      "total:\t199\t1.00\n",
      "\n",
      "---\tБуряад аман зохёол\n",
      "\n",
      "ru\t245\t0.96\n",
      "uk\t1\t0.0\n",
      "en\t0\t0.0\n",
      "bxr\t9\t0.04\n",
      "total:\t255\t1.00\n",
      "\n",
      "---\tБаргажанай буряадууд (Хурамхаан, Баргажан)\n",
      "\n",
      "ru\t210\t0.91\n",
      "uk\t1\t0.0\n",
      "en\t0\t0.0\n",
      "bxr\t21\t0.09\n",
      "total:\t232\t1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lang_detects_freqs = {}\n",
    "\n",
    "for pub in corpora:\n",
    "    lang_detects_freqs[pub] = {}\n",
    "    for text in corpora[pub]:\n",
    "        options = simple_lang_detect(freqs, text)\n",
    "        if len(options) > 0:\n",
    "            l = options[0][0]\n",
    "            if l in lang_detects_freqs[pub]:\n",
    "                lang_detects_freqs[pub][l] += 1\n",
    "            else:\n",
    "                lang_detects_freqs[pub][l] = 1\n",
    "\n",
    "for pub in lang_detects_freqs:\n",
    "    print('---\\t' + pub + '\\n')\n",
    "    p_info = lang_detects_freqs[pub]\n",
    "    total = 0\n",
    "    for l in p_info:\n",
    "        total += p_info[l]        \n",
    "    for l in test_langs:\n",
    "        if l in p_info:\n",
    "            k = round(p_info[l] / total, 2)\n",
    "            q = p_info[l]\n",
    "        else:\n",
    "            k = 0.00\n",
    "            q = 0\n",
    "        print('%s\\t%d\\t%s' % (l, q, str(k)))\n",
    "    print('total:\\t%d\\t%s\\n' % (total, '1.00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте то же самое для определителя на символьных энграммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\tкиберпанк, который мы заслужили\n",
      "\n",
      "ru\t1077\t0.91\n",
      "uk\t0\t0.0\n",
      "en\t44\t0.04\n",
      "bxr\t59\t0.05\n",
      "total:\t1180\t1.00\n",
      "\n",
      "---\teternal classic\n",
      "\n",
      "ru\t0\t0.0\n",
      "uk\t0\t0.0\n",
      "en\t726\t1.0\n",
      "bxr\t2\t0.0\n",
      "total:\t728\t1.00\n",
      "\n",
      "---\tФайні меми про українську лiтературу\n",
      "\n",
      "ru\t131\t0.53\n",
      "uk\t97\t0.39\n",
      "en\t6\t0.02\n",
      "bxr\t15\t0.06\n",
      "total:\t249\t1.00\n",
      "\n",
      "---\tUkraїner\n",
      "\n",
      "ru\t85\t0.43\n",
      "uk\t94\t0.47\n",
      "en\t4\t0.02\n",
      "bxr\t16\t0.08\n",
      "total:\t199\t1.00\n",
      "\n",
      "---\tБуряад аман зохёол\n",
      "\n",
      "ru\t36\t0.14\n",
      "uk\t0\t0.0\n",
      "en\t0\t0.0\n",
      "bxr\t219\t0.86\n",
      "total:\t255\t1.00\n",
      "\n",
      "---\tБаргажанай буряадууд (Хурамхаан, Баргажан)\n",
      "\n",
      "ru\t122\t0.53\n",
      "uk\t0\t0.0\n",
      "en\t1\t0.0\n",
      "bxr\t109\t0.47\n",
      "total:\t232\t1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pub in corpora:\n",
    "    lang_ratio = Counter(clf.predict(corpora[pub]))\n",
    "    \n",
    "    print('---\\t' + pub + '\\n')\n",
    "    total = 0\n",
    "    for lang in lang_ratio:\n",
    "        total += lang_ratio[lang]        \n",
    "    for lang in test_langs:\n",
    "        if lang in lang_ratio:\n",
    "            k = round(lang_ratio[lang] / total, 2)\n",
    "            q = lang_ratio[lang]\n",
    "        else:\n",
    "            k = 0.00\n",
    "            q = 0\n",
    "        print('%s\\t%d\\t%s' % (lang, q, str(k)))\n",
    "    print('total:\\t%d\\t%s\\n' % (total, '1.00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обсудите работу каждого из классификаторов, обсудите ошибки, объясните разницу в результатах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определитель по частотным словам может ориентироваться на числа, слова из пары символов или смайлики. Поэтому энграммный определитель выглядит точнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
